{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Parsing with NLTK and Foma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2017 by [Damir Cavar](http://cavar.me/damir/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial related to the discussion of grammar engineering and parsing in the class *Alternative Syntactic Theories* taught at Indiana University in the Linguistics Department in Spring 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code examples require an installed *foma.py* module. Since I am using [Python 3.x](https://www.python.org/downloads/) here, I would recommend to use my version of *foma.py* and install it in the local modules folder of your Python distribution. In my case, since I use [Anaconda](https://www.continuum.io/downloads), the file goes in *anaconda/lib/python3.5/* in my home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammars and Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NLTK in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can declare a feature structure and display it using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 3     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n"
     ]
    }
   ],
   "source": [
    "fstr = nltk.FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\")\n",
    "print(fstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use Feature grammars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.grammar import FeatureGrammar, FeatDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a feature grammar in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grammarText = \"\"\"\n",
    "% start S\n",
    "# ############################\n",
    "# Grammar Rules\n",
    "# ############################\n",
    "S -> NP[NUM=?n, PERS=?p, CASE='nom'] VP[NUM=?n, PERS=?p]\n",
    "NP[NUM=?n, PERS=?p, CASE=?c] -> N[NUM=?n, PERS=?p, CASE=?c]\n",
    "NP[NUM=?n, PERS=?p, CASE=?c] -> D[NUM=?n,CASE=?c] NC[NUM=?n,PERS=?p,CASE=?c]\n",
    "NP[NUM=?n, PERS=?p, CASE=?c] -> Pron[NUM=?n,PERS=?p,CASE=?c]\n",
    "VP[NUM=?n, PERS=?p] -> V[NUM=?n, PERS=?p]\n",
    "VP[NUM=?n, PERS=?p] -> V[NUM=?n, PERS=?p] NP[CASE='acc']\n",
    "\"\"\"\n",
    "\n",
    "grammar = FeatureGrammar.fromstring(grammarText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the grammar with the input sentence *John loves Mary* fails, because there is not lexical defintion of these entries in the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'John', 'loves', 'Mary'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5fb351ae764b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatureChartParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"John loves Mary\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchart_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/parse/chart.py\u001b[0m in \u001b[0;36mchart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_coverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0mchart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chart_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/grammar.py\u001b[0m in \u001b[0;36mcheck_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             raise ValueError(\"Grammar does not cover some of the \"\n\u001b[0;32m--> 631\u001b[0;31m                              \"input words: %r.\" % missing)\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_grammar_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'John', 'loves', 'Mary'\"."
     ]
    }
   ],
   "source": [
    "parser = nltk.parse.FeatureChartParser(grammar)\n",
    "sentence = \"John loves Mary\"\n",
    "result = list(parser.parse(sentence.split()))\n",
    "if result:\n",
    "    for x in result:\n",
    "        print(x)\n",
    "else:\n",
    "    print(\"*\", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include foma and a morphology using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import foma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line will load the *eng.fst* Foma morphology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fst = foma.FST.load(b'eng.fst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the analysis for each single token by submitting it to the FST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John+N+Sg+Masc+NEPerson\n",
      "love+V+3P+Sg\n",
      "Mary+N+Sg+Fem+NEPerson\n"
     ]
    }
   ],
   "source": [
    "tokens = \"John loves Mary\".split()\n",
    "for token in tokens:\n",
    "    result = list(fst.apply_up(str.encode(token)))\n",
    "    for r in result:\n",
    "        print(r.decode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to convert the flat string annotation from the morphology to a NLTK feature structure, we need to translate some entires to a corresponding Attribute Value Matrix (AVM). In the following table we define a feature in the morphology output and the corresponging feature structure that it corresponds with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureMapping = {\n",
    "    'Def'   : \"DETTYPE = def\",\n",
    "    'Indef' : \"DETTYPE = indef\",\n",
    "    'Sg'    : \"NUM = sg\",\n",
    "    'Pl'    : \"NUM = pl\",\n",
    "    '3P'    : \"PERS = 3\",\n",
    "    'Masc'  : \"GENDSEM = male\",\n",
    "    'Fem'   : \"GENDSEM = female\",\n",
    "    'Dat'   : \"CASE = dat\",\n",
    "    'Acc'   : \"CASE = acc\",\n",
    "    'Nom'   : \"CASE = nom\",\n",
    "    'NEPersonName' : \"\"\"NTYPE = [NSYN = proper,\n",
    "                                NSEM = [PROPER = [PROPERTYPE = name,\n",
    "                                                 NAMETYPE   = first_name]]],\n",
    "                        HUMAN = 1\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a function *feat2LFG* to convert the feature tag in the morphological analysis to a LFG-compatible AVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat2LFG(f):\n",
    "    result = featureMapping.get(f, \"\")\n",
    "    return(nltk.FeatStruct(\"\".join( (\"[\", result, \"]\") )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helper function is recursive. It mapps the AVM to a bracketed string annotation of feature structures, as used in the NLTK feature grammar format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatFStructure(f):\n",
    "    res = \"\"\n",
    "    for key in f.keys():\n",
    "        val = f[key]\n",
    "        if res:\n",
    "            res += ', '\n",
    "        if (isinstance(val, FeatDict)):\n",
    "            res += key + '=' + flatFStructure(val)\n",
    "        else:\n",
    "            res += key + \"=\" + str(val)\n",
    "    return('[' + res + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a parse function that prints out parse trees for an input, maintaining the extended feature structures at the lexical level. It can now parse sentences that contain words that are not specified as lexical words in the grammar, but rather as paths in the morphological finite state transducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseFoma(sentence):\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    tokenAnalyses = {}\n",
    "    rules = []\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        aVal = []\n",
    "        result = list(fst.apply_up(str.encode(token)))\n",
    "        for r in result:\n",
    "            elements = r.decode('utf8').split('+')\n",
    "\n",
    "            lemma = elements[0]\n",
    "            tokFeat = nltk.FeatStruct(\"[PRED=\" + lemma + \"]\")\n",
    "\n",
    "            cat = elements[1]\n",
    "            if len(elements) > 2:\n",
    "                feats = tuple(elements[2:])\n",
    "            else:\n",
    "                feats = ()\n",
    "            for x in feats:\n",
    "                fRes2 = feat2LFG(x)\n",
    "                fRes = tokFeat.unify(fRes2)\n",
    "                if fRes:\n",
    "                    tokFeat = fRes\n",
    "                else:\n",
    "                    print(\"Error unifying:\", tokFeat, fRes2)\n",
    "            flatFStr = flatFStructure(tokFeat)\n",
    "            aVal.append(cat + flatFStr)\n",
    "            rules.append(cat + flatFStr + \" -> \" + \"'\" + token + \"'\")\n",
    "        tokenAnalyses[count] = aVal\n",
    "        count += 1\n",
    "\n",
    "    grammarText2 = grammarText + \"\\n\" + \"\\n\".join(rules)\n",
    "\n",
    "    grammar = FeatureGrammar.fromstring(grammarText2)\n",
    "    parser = nltk.parse.FeatureChartParser(grammar)\n",
    "    result = list(parser.parse(tokens))\n",
    "    if result:\n",
    "        for x in result:\n",
    "            print(x)\n",
    "    else:\n",
    "        print(\"*\", sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call this function using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[]\n",
      "  (NP[CASE=?c, NUM='sg', PERS=?p]\n",
      "    (N[GENDSEM='male', NUM='sg', PRED='John'] John))\n",
      "  (VP[NUM='sg', PERS=3]\n",
      "    (V[NUM='sg', PERS=3, PRED='love'] loves)\n",
      "    (NP[CASE='acc', NUM='sg', PERS=3]\n",
      "      (Pron[CASE='acc', GENDSEM='female', NUM='sg', PERS=3, PRED='she']\n",
      "        her))))\n"
     ]
    }
   ],
   "source": [
    "parseFoma(\"John loves her\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
