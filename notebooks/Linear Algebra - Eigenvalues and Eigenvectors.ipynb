{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra: Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2018 by [Damir Cavar](http://damir.cavar.me/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version:** 1.0, January 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial related to the L665 course on Machine Learning for NLP focusing on Deep Learning, Fall 2018 at Indiana University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following material is based on *Linear Algebra Review and Reference* by Zico Kolter (updated by Chuong Do) from September 30, 2015. This means, many passages are literally copied, many are rewritten. I do not mark sections that are added or different. Consider this notebook a extended annotation of Kolter's (and Do's) notes. See also James E. Gentle (2017) [Matrix Algebra: Theory, Computations and Applications in Statistics](http://www.springer.com/us/book/9780387708720). Second edition. Springer. Another good resource is Philip N. Klein (2013) [Coding the Matrix: Linear Algebra through Applications to Computer Science](http://codingthematrix.com/), Newtonian Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See for the introduction the Notebook \"Linear Algebra\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic definition:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $A \\in \\mathbb{R}^{n\\times n}$, $\\lambda$ is the **eigenvalue** of $A$, if there is a non-zero vector $x$, the corresponding **eigenvector**, if the following is true:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Ax = \\lambda x, x \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, given a square matrix $A \\in \\mathbb{R}^{n\\times n}$, we say that $\\lambda \\in \\mathbb{C}$ is an **eigenvalue** of $A$ and $x \\in \\mathbb{C}^n$ is the corresponding **eigenvector**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, this definition means that multiplying $A$ by the vector $x$ results in a new vector that points in the same direction as $x$, but is scaled by a factor $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that for any **eigenvector** $x \\in \\mathbb{C}^n$, and scalar $t \\in \\mathbb{C}, A(cx) = cAx = c\\lambda x = \\lambda(cx)$, so $cx$ is also an **eigenvector**. For this reason when we talk about **“the” eigenvector** associated with $\\lambda$, we usually assume that the **eigenvector** is normalized to have length $1$ (this still creates some ambiguity, since $x$ and $−x$ will both be **eigenvectors**, but we will have to live with this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the equation above to state that $(\\lambda, x)$ is an eigenvalue-eigenvector pair of $A$ if:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(\\lambda I − A)x = 0, x \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But $(\\lambda I − A)x = 0$ has a non-zero solution to $x$ if and only if $(\\lambda I − A)$ has a non-empty nullspace, which is only the case if $(\\lambda I − A)$ is singular:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$|(\\lambda I − A)| = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the previous definition of the determinant to expand this expression\n",
    "into a (very large) polynomial in $\\lambda$, where $\\lambda$ will have maximum degree $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then find the $n$ (possibly complex) roots of this polynomial to find the $n$ eigenvalues $\\lambda_1, \\dots{}, \\lambda_n$. To find the eigenvector corresponding to the eigenvalue $\\lambda_i$, we simply solve the linear equation $(\\lambda_iI − A)x = 0$. It should be noted that this is not the method which is actually used in practice to numerically compute the eigenvalues and eigenvectors (remember that the complete expansion of the determinant has $n!$ terms); it is rather a mathematical argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are properties of eigenvalues and eigenvectors (in all cases assume $A \\in \\mathbb{R}^{n\\times n}$ has eigenvalues $\\lambda_i, \\dots{}, \\lambda_n$ and associated eigenvectors $x_1, \\dots{}, x_n$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The trace of a $A$ is equal to the sum of its eigenvalues,<br/>\n",
    "$\\mathrm{tr}A = \\sum_{i=1}^n \\lambda_i$\n",
    "- The determinant of $A$ is equal to the product of its eigenvalues,<br/>\n",
    "$|A| = \\prod_{i=1}^n \\lambda_i$\n",
    "- The rank of $A$ is equal to the number of non-zero eigenvalues of $A$\n",
    "- If $A$ is non-singular then $1/\\lambda_i$ is an eigenvalue of $A^{−1}$ with associated eigenvector $x_i$, i.e., $A^{−1}x_i = (1/\\lambda_i)x_i$. (To prove this, take the eigenvector equation, $Ax_i = \\lambda_i x_i$ and left-multiply each side by $A^{−1}$.)\n",
    "- The eigenvalues of a diagonal matrix $D = \\mathrm{diag}(d_1, \\dots{}, d_n)$ are just the diagonal entries $d_1, \\dots{}, d_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write all the eigenvector equations simultaneously as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A X = X \\Lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the columns of $X \\in \\mathbb{R}^{n\\times n}$ are the eigenvectors of $A$ and $\\Lambda$ is a diagonal matrix whose entries are the eigenvalues of $A$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\in \\mathbb{R}^{n\\times n} = \\begin{bmatrix}\n",
    " \\big| & \\big| &  & \\big| \\\\[0.3em]\n",
    " x_1 & x_2 & \\cdots & x_n \\\\[0.3em]\n",
    " \\big| & \\big| &  & \\big|  \n",
    "\\end{bmatrix} , \\Lambda = \\mathrm{diag}(\\lambda_1, \\dots{}, \\lambda_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the eigenvectors of $A$ are linearly independent, then the matrix $X$ will be invertible, so $A = X \\Lambda X^{−1}$. A matrix that can be written in this form is called **diagonalizable**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors of Symmetric Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two remarkable properties come about when we look at the eigenvalues and eigenvectors\n",
    "of a symmetric matrix $A \\in \\mathbb{S}^n$.\n",
    "\n",
    "1. it can be shown that all the eigenvalues of $A$ are real\n",
    "2. the eigenvectors of $A$ are orthonormal, i.e., the matrix $X$ defined above is an orthogonal matrix (for this reason, we denote the matrix of eigenvectors as $U$ in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore represent $A$ as $A = U \\Lambda U^T$, remembering from above that the inverse of an orthogonal matrix is just its transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, we can show that the definiteness of a matrix depends entirely on the sign of\n",
    "its eigenvalues. Suppose $A \\in \\mathbb{S}^n = U \\Lambda U^T$. Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x^T A x = x^T U \\Lambda U^T x = y^T \\Lambda y = \\sum_{i=1}^n \\lambda_i y^2_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y = U^T x$ (and since $U$ is full rank, any vector $y \\in \\mathbb{R}^n$ can be represented in this form)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $y^2_i$ is always positive, the sign of this expression depends entirely on the $\\lambda_i$'s. If all $\\lambda_i > 0$, then the matrix is positive definite; if all $\\lambda_i \\leq 0$, it is positive semidefinite. Likewise, if all $\\lambda_i < 0$ or $\\lambda_i \\leq 0$, then $A$ is negative definite or negative semidefinite respectively. Finally, if $A$ has both positive and negative eigenvalues, it is indefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An application where eigenvalues and eigenvectors come up frequently is in maximizing\n",
    "some function of a matrix. In particular, for a matrix $A \\in \\mathbb{S}^n$, consider the following maximization problem,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{max}_{x\\in \\mathbb{R}^n} x^T A x \\mbox{ subject to } \\|x\\|^2_2 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e., we want to find the vector (of norm $1$) which maximizes the quadratic form. Assuming\n",
    "the eigenvalues are ordered as $\\lambda_1 \\geq \\lambda_2 \\geq \\dots{} \\geq \\lambda_n$, the optimal $x$ for this optimization problem is $x_1$, the eigenvector corresponding to $\\lambda_1$. In this case the maximal value of the quadratic form is $\\lambda_1$. Similarly, the optimal solution to the minimization problem,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{min}_{x\\in \\mathbb{R}^n} x^T A x \\mbox{ subject to } \\|x\\|^2_2 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is $x_n$, the eigenvector corresponding to $\\lambda_n$, and the minimal value is $\\lambda_n$. This can be proved by appealing to the eigenvector-eigenvalue form of $A$ and the properties of orthogonal matrices. However, in the next section we will see a way of showing it directly using matrix calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
