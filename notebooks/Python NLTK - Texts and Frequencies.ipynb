{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python NLTK: Texts and Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(C) 2017-2019 by [Damir Cavar](http://damir.cavar.me/) <<dcavar@iu.edu>>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version:** 0.4, September 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download:** This and various other Jupyter notebooks are available from my [GitHub repo](https://github.com/dcavar/python-tutorial-for-ipython)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/) ([CA BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a brief introduction to NLTK for simple frequency analysis of texts. I created this notebook for intro to corpus linguistics and natural language processing classes at Indiana University between 2017 and 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, in the folder with the notebook we expect a subfolder data that contains a file HOPG.txt. This file contains the novel \"A House of Pomegranates\" by Oscar Wilde taken as raw text from [Project Gutenberg](https://www.gutenberg.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple File Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a text into memory in Python is faily simple. We open a file, read from it, and close the file again. The following code prints out the first 300 characters of the text in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A HOUSE OF POMEGRANATES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents:\n",
      "\n",
      "The Young King\n",
      "The Birthday of the Infanta\n",
      "The Fisherman and his Soul\n",
      "The Star-child\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE YOUNG KING\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[TO MARGARET LADY BROOKE--THE RANEE OF SARAWAK]\n",
      "\n",
      "\n",
      "It was the night before the day fixed for his coronation, and the\n",
      "young King was sitting alone in his b ...\n"
     ]
    }
   ],
   "source": [
    "ifile = open(\"HOPG.txt\", mode='r', encoding='utf-8')\n",
    "text = ifile.read()\n",
    "ifile.close()\n",
    "print(text[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optional parameters in the *open* function above define the **mode** of operations on the file and the **encoding** of the content. For example, setting the **mode** to **r** declares that *reading* from the file is the only permitted operation that we will perform in the following code. Setting the **encoding** to **utf-8** declares that all characters will be encoded using the [Unicode](https://en.wikipedia.org/wiki/Unicode) encoding schema [UTF-8](https://en.wikipedia.org/wiki/UTF-8) for the content of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the [NLTK](https://www.nltk.org/) module in Python to work with frequency profiles and [n-grams](https://en.wikipedia.org/wiki/N-gram) using the tokens or words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now lower the text, which means normalizing it to all characters lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a house of pomegranates\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "contents:\n",
      "\n",
      "the young king\n",
      "the birthday of the infanta\n",
      "the fisherman and his soul\n",
      "the star-child\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the young king\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[to margaret lady brooke--the ranee of sarawak]\n",
      "\n",
      "\n",
      "it was the night before the day fixed for his coronation, and the\n",
      "young king was sitting alone in his b ...\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a frequency profile from the text file, we can use the [NLTK](https://www.nltk.org/) function *FreqDist*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFD = nltk.FreqDist(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove certain characters from the distribution, or alternatively replace these characters in the text variable. The following loop removes them from the frequency profile in myFD, which is a dictionary data structure in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in \":,.-[];!'\\\"\\t\\n/ ?\":\n",
    "    del myFD[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the frequency profile by looping through the returned data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 11231\n",
      "h 10802\n",
      "o 9408\n",
      "u 3269\n",
      "s 8093\n",
      "e 17372\n",
      "f 3089\n",
      "p 1884\n",
      "m 3271\n",
      "g 2666\n",
      "r 7603\n",
      "n 8843\n",
      "t 12521\n",
      "c 2693\n",
      "y 2168\n",
      "k 1026\n",
      "i 8307\n",
      "b 1812\n",
      "d 7249\n",
      "l 5270\n",
      "w 3665\n",
      "x 48\n",
      "v 1122\n",
      "q 81\n",
      "j 103\n",
      "z 61\n"
     ]
    }
   ],
   "source": [
    "for x in myFD:\n",
    "    print(x, myFD[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To relativize the frequencies, we need to compute the total number of characters. This is assuming that we removed all punctuation symbols. The frequency distribution instance myFD provides a method to access the values associated with the individual characters. This will return a list of values, that is the frequencies associated with the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([11231, 10802, 9408, 3269, 8093, 17372, 3089, 1884, 3271, 2666, 7603, 8843, 12521, 2693, 2168, 1026, 8307, 1812, 7249, 5270, 3665, 48, 1122, 81, 103, 61])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFD.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *sum* function can summarize these values in its list argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133657"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(myFD.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid type problems when we compute the relative frequency of characters, we can convert the total number of characters into a *float*. This will guarantee that the division in the following relativisation step will be a *float* as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133657.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(sum(myFD.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the resulting number of characters in the *total* variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133657.0\n"
     ]
    }
   ],
   "source": [
    "total = float(sum(myFD.values()))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate a probability distribution over characters. To convert the frequencies into relative frequencies we use list comprehension and divide every single list element by total. The resulting relative frequencies are stored in the variable *relfreq*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08402852076584093, 0.0808188123330615, 0.07038913038598801, 0.024458127894536014, 0.06055051362816762, 0.1299744869329702, 0.02311139708357961, 0.014095782488010355, 0.024473091570213306, 0.019946579677832064, 0.056884413087230745, 0.06616189200715264, 0.09368009157769515, 0.020148589299475522, 0.016220624434186013, 0.007676365622451499, 0.06215162692563801, 0.013557090163627793, 0.05423584249234982, 0.03942928540966803, 0.0274209356786401, 0.0003591282162550409, 0.008394622054961581, 0.0006060288649303815, 0.0007706292973806085, 0.0004563921081574478]\n"
     ]
    }
   ],
   "source": [
    "relfrq = [ x/total for x in myFD.values() ]\n",
    "print(relfrq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) for the character distribution using the relative frequencies. We will need the [logarithm](https://en.wikipedia.org/wiki/Logarithm) function from the Python *math* module for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) function according to the equation $I = - \\sum P(x) log_2( P(x) )$ as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -sum( [ x * log(x, 2) for x in p ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the entropy of the character distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.124824125135032\n"
     ]
    }
   ],
   "source": [
    "print(entropy(relfrq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested in the point-wise entropy of the characters in this distribution, thus needing the entropy of each single character. We can compute that in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3002319806578157, 0.29330480822209687, 0.2694850339615855, 0.13093762006835424, 0.24497024185626542, 0.38260584969385675, 0.12561626066788154, 0.08666922421352652, 0.13099613411450642, 0.11265259339509692, 0.2352638524022937, 0.2592127456210649, 0.32002184459468397, 0.11350057702872672, 0.09644826809662171, 0.053929238563860095, 0.24910770047189124, 0.08411915007238721, 0.2280405439219216, 0.18392139623527803, 0.1422756742696596, 0.004109580806277946, 0.05789199083219161, 0.006477433994507777, 0.007969598004767611, 0.005064783367910896]\n"
     ]
    }
   ],
   "source": [
    "entdist = [ -x * log(x, 2) for x in relfrq ]\n",
    "print(entdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now compute the variance over this point-wise entropy distribution or other properties of the frequency distribution as for example median, mode, or standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Characters to Words/Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the frequency profile is for the characters in the text, not the words or tokens. In order to generate a frequency profile over words/tokens in the text, we need to utilize a **tokenizer**. [NLTK](https://www.nltk.org/) provides basic tokenization functions. We will use the *word_tokenize* function to generate a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the first 20 tokens to verify our data structure is a list with lower-case strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'house',\n",
       " 'of',\n",
       " 'pomegranates',\n",
       " 'contents',\n",
       " ':',\n",
       " 'the',\n",
       " 'young',\n",
       " 'king',\n",
       " 'the',\n",
       " 'birthday',\n",
       " 'of',\n",
       " 'the',\n",
       " 'infanta',\n",
       " 'the',\n",
       " 'fisherman',\n",
       " 'and',\n",
       " 'his',\n",
       " 'soul',\n",
       " 'the']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate a frequency profile from the token list, as we did with the characters above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTokenFD = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency profile can be printed out in the same way as above by looping over the tokens and their frequencies. Note that we restrict the loop to the first 20 tokens here just to keep the notebook smaller. You can remove the [:20] selector in your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 673\n",
      "house 26\n",
      "of 1108\n",
      "pomegranates 6\n",
      "contents 1\n",
      ": 33\n",
      "the 2552\n",
      "young 112\n",
      "king 70\n",
      "birthday 8\n",
      "infanta 37\n",
      "fisherman 76\n",
      "and 2118\n",
      "his 483\n",
      "soul 99\n",
      "star-child 42\n",
      "[ 4\n",
      "to 697\n",
      "margaret 1\n",
      "lady 4\n"
     ]
    }
   ],
   "source": [
    "for token in list(myTokenFD.items())[:20]:\n",
    "    print(token[0], token[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK](https://www.nltk.org/) provides simple methods to generate [n-gram](https://en.wikipedia.org/wiki/N-gram) models or frequency profiles over [n-grams](https://en.wikipedia.org/wiki/N-gram) from any kind of list or sequence. We can for example generate a bi-gram model, that is an [n-grams](https://en.wikipedia.org/wiki/N-gram) model for n = 2, from the text tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTokenBigrams = nltk.ngrams(tokens, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store the bigrams in a list that we want to process and analyze further, we convert the **Python generator object** myTokenBigrams to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(myTokenBigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify that the resulting data structure is indeed a list of string tuples. We will print out the first 20 tuples from the bigram list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'house'), ('house', 'of'), ('of', 'pomegranates'), ('pomegranates', 'contents'), ('contents', ':'), (':', 'the'), ('the', 'young'), ('young', 'king'), ('king', 'the'), ('the', 'birthday'), ('birthday', 'of'), ('of', 'the'), ('the', 'infanta'), ('infanta', 'the'), ('the', 'fisherman'), ('fisherman', 'and'), ('and', 'his'), ('his', 'soul'), ('soul', 'the'), ('the', 'star-child')]\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify the number of bigrams and check that there are exactly *number of tokens - 1 = number of bigrams* in the resulting list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38126\n",
      "38127\n"
     ]
    }
   ],
   "source": [
    "print(len(bigrams))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency profile from these bigrams is generated in exactly the same way as from the token list in the examples above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBigramFD = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would want to know some more general properties of the frequency distribution, we can print out information about it. The print statement for this bigram frequency distribution tells us that we have 17,766 types and 38,126 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 17766 samples and 38126 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(myBigramFD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigrams and their corresponding frequencies can be printed using a *for* loop. We restrict the number of printed items to 20, just to keep this list reasonably long. If you would like to see the full frequency profile, remove the [:20] restrictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'house') 4\n",
      "('house', 'of') 5\n",
      "('of', 'pomegranates') 4\n",
      "('pomegranates', 'contents') 1\n",
      "('contents', ':') 1\n",
      "(':', 'the') 2\n",
      "('the', 'young') 107\n",
      "('young', 'king') 24\n",
      "('king', 'the') 1\n",
      "('the', 'birthday') 3\n",
      "('birthday', 'of') 3\n",
      "('of', 'the') 354\n",
      "('the', 'infanta') 33\n",
      "('infanta', 'the') 1\n",
      "('the', 'fisherman') 5\n",
      "('fisherman', 'and') 3\n",
      "('and', 'his') 53\n",
      "('his', 'soul') 51\n",
      "('soul', 'the') 1\n",
      "('the', 'star-child') 41\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for bigram in list(myBigramFD.items())[:20]:\n",
    "    print(bigram[0], bigram[1])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty printing the bigrams is possible as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a house 4\n",
      "house of 5\n",
      "of pomegranates 4\n",
      "pomegranates contents 1\n",
      "contents : 1\n",
      ": the 2\n",
      "the young 107\n",
      "young king 24\n",
      "king the 1\n",
      "the birthday 3\n",
      "birthday of 3\n",
      "of the 354\n",
      "the infanta 33\n",
      "infanta the 1\n",
      "the fisherman 5\n",
      "fisherman and 3\n",
      "and his 53\n",
      "his soul 51\n",
      "soul the 1\n",
      "the star-child 41\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for ngram in list(myBigramFD.items())[:20]:\n",
    "    print(\" \".join(ngram[0]), ngram[1])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remove the [:20] restrictor above and print out the entire frequency profile. If you select and copy the profile to your clipboard, you can paste it into your favorite spreadsheet software and sort, analyze, and study the distribution in many interesting ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running the frequency profile through a loop we can also use a list comprehension construction in Python to generate a list of tuples with the n-gram and its frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a house', 4), ('house of', 5), ('of pomegranates', 4), ('pomegranates contents', 1), ('contents :', 1), (': the', 2), ('the young', 107), ('young king', 24), ('king the', 1), ('the birthday', 3), ('birthday of', 3), ('of the', 354), ('the infanta', 33), ('infanta the', 1), ('the fisherman', 5), ('fisherman and', 3), ('and his', 53), ('his soul', 51), ('soul the', 1), ('the star-child', 41), ('star-child the', 1), ('king [', 1), ('[ to', 4), ('to margaret', 1), ('margaret lady', 1), ('lady brooke', 1), ('brooke --', 1), ('-- the', 4), ('the ranee', 1), ('ranee of', 1), ('of sarawak', 1), ('sarawak ]', 1), ('] it', 2), ('it was', 52), ('was the', 20), ('the night', 3), ('night before', 1), ('before the', 8), ('the day', 7), ('day fixed', 1), ('fixed for', 1), ('for his', 7), ('his coronation', 2), ('coronation ,', 3), (', and', 1271), ('and the', 287), ('king was', 1), ('was sitting', 1), ('sitting alone', 1), ('alone in', 1), ('in his', 31), ('his beautiful', 1), ('beautiful chamber', 1), ('chamber .', 1), ('. his', 8), ('his courtiers', 1), ('courtiers had', 1), ('had all', 2), ('all taken', 1), ('taken their', 1), ('their leave', 1), ('leave of', 1), ('of him', 8), ('him ,', 146), (', bowing', 1), ('bowing their', 1), ('their heads', 5), ('heads to', 1), ('to the', 149), ('the ground', 15), ('ground ,', 6), (', according', 1), ('according to', 1), ('the ceremonious', 1), ('ceremonious usage', 1), ('usage of', 1), ('day ,', 3), ('and had', 9), ('had retired', 2), ('retired to', 2), ('the great', 25), ('great hall', 2), ('hall of', 1), ('the palace', 18), ('palace ,', 5), (', to', 6), ('to receive', 1), ('receive a', 1), ('a few', 9), ('few last', 1), ('last lessons', 1), ('lessons from', 1), ('from the', 69), ('the professor', 1), ('professor of', 1), ('of etiquette', 1), ('etiquette ;', 1), ('; there', 2), ('there being', 1), ('being some', 1)]\n"
     ]
    }
   ],
   "source": [
    "ngrams = [ (\" \".join(ngram), myBigramFD[ngram]) for ngram in myBigramFD ]\n",
    "print(ngrams[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate an increasing frequency profile using the sort function on the second element of the tuple list, that is on the frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pomegranates contents', 1), ('contents :', 1), ('king the', 1), ('infanta the', 1), ('soul the', 1), ('star-child the', 1), ('king [', 1), ('to margaret', 1), ('margaret lady', 1), ('lady brooke', 1), ('brooke --', 1), ('the ranee', 1), ('ranee of', 1), ('of sarawak', 1), ('sarawak ]', 1), ('night before', 1), ('day fixed', 1), ('fixed for', 1), ('king was', 1), ('was sitting', 1)]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "sortedngrams = sorted(ngrams, key=lambda x: x[1])\n",
    "print(sortedngrams[:20])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the speed of this *sorted* call by using the *itemgetter()* function in the *operator* module. Let us import this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the sort-key for *sorted* using the *itemgetter* function and selecting with 1 the second element in the tuple. Remember that the enumeration of elements in lists or tuples in Python starts at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pomegranates contents', 1), ('contents :', 1), ('king the', 1), ('infanta the', 1), ('soul the', 1), ('star-child the', 1), ('king [', 1), ('to margaret', 1), ('margaret lady', 1), ('lady brooke', 1), ('brooke --', 1), ('the ranee', 1), ('ranee of', 1), ('of sarawak', 1), ('sarawak ]', 1), ('night before', 1), ('day fixed', 1), ('fixed for', 1), ('king was', 1), ('was sitting', 1)]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "sortedngrams = sorted(ngrams, key=itemgetter(1))\n",
    "print(sortedngrams[:20])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decreasing frequency profile can be generated using another parameter to *sorted*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(', and', 1271), ('of the', 354), ('and the', 287), ('. and', 205), (\". '\", 193), ('in the', 158), (\", '\", 155), ('to the', 149), ('him ,', 146), ('. the', 112), (\"' and\", 108), ('the young', 107), (', but', 96), ('and he', 89), (', for', 85), (\"? '\", 84), ('on the', 80), ('said to', 79), ('to him', 77), ('and said', 77)]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "sortedngrams = sorted(ngrams, key=itemgetter(1), reverse=True)\n",
    "print(sortedngrams[:20])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pretty-print the decreasing frequency profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and 1271\n",
      "of the 354\n",
      "and the 287\n",
      ". and 205\n",
      ". ' 193\n",
      "in the 158\n",
      ", ' 155\n",
      "to the 149\n",
      "him , 146\n",
      ". the 112\n",
      "' and 108\n",
      "the young 107\n",
      ", but 96\n",
      "and he 89\n",
      ", for 85\n",
      "? ' 84\n",
      "on the 80\n",
      "said to 79\n",
      "to him 77\n",
      "and said 77\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "sortedngrams = sorted(ngrams, key=itemgetter(1), reverse=True)\n",
    "for t in sortedngrams[:20]:\n",
    "    print(t[0], t[1])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) 2017-2019 by [Damir Cavar](http://damir.cavar.me/) <<dcavar@iu.edu>>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
