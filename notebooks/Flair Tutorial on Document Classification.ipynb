{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njeqYj-FiSZW"
   },
   "source": [
    "# Flair Tutorial on Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a57G2gLkh6Jn"
   },
   "source": [
    "(C) 2019 by [Damir Cavar](http://damir.cavar.me/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a script to split a corpus into a training, development, and test corpus. The corpus format will use the [FastText](https://fasttext.cc/docs/en/supervised-tutorial.html) format. We will split the corpus into:\n",
    "\n",
    "- training set\n",
    "- development set\n",
    "- test set\n",
    "\n",
    "We will use the dev data for measuring over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.data import TaggedCorpus\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the corpus files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the corpus files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 10:08:37,254 Reading data from data\n",
      "2019-04-10 10:08:37,255 Train: data/train.txt\n",
      "2019-04-10 10:08:37,255 Dev: data/dev.txt\n",
      "2019-04-10 10:08:37,256 Test: data/test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder,\n",
    "                                                                     test_file='test.txt',\n",
    "                                                                     dev_file='dev.txt',\n",
    "                                                                     train_file='train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some stats for the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 8000,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"1\": 4099,\n",
      "            \"2\": 3901\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 737984,\n",
      "            \"min\": 17,\n",
      "            \"max\": 252,\n",
      "            \"avg\": 92.248\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 1000,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"2\": 508,\n",
      "            \"1\": 492\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 92053,\n",
      "            \"min\": 18,\n",
      "            \"max\": 228,\n",
      "            \"avg\": 92.053\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 1000,\n",
      "        \"number_of_documents_per_class\": {\n",
      "            \"1\": 506,\n",
      "            \"2\": 494\n",
      "        },\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 89541,\n",
      "            \"min\": 20,\n",
      "            \"max\": 220,\n",
      "            \"avg\": 89.541\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "stats = corpus.obtain_statistics()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the modules for training a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTask\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a label dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the different word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                   FlairEmbeddings('news-forward'),\n",
    "                   FlairEmbeddings('news-backward'),\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three embedding models will be concatenated and should give state of the art results. If this is too slow and complicated on your computer, try first without the *FlairEmbeddings*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Embeddings generate one embedding for an entire text. The produced embeddings are PyTorch vectors. There are two different methods using the word embeddings to obtain a document embedding.\n",
    "- Pooling Operation\n",
    "- RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pooling Operation** calculates a pooling operation over all word embeddings in a document. The default operation is mean which gives us the mean of all words in the sentence. The resulting embedding is taken as document embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a mean document embedding simply create any number of TokenEmbeddings first and put them in a list. Afterwards, initiate the DocumentPoolEmbeddings with this list of TokenEmbeddings. If you want to create a document embedding using GloVe embeddings together with CharLMEmbeddings, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
    "\n",
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
    "                                              flair_embedding_backward,\n",
    "                                              flair_embedding_forward])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create an example sentence and call the embedding's embed() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3197,  0.2621,  0.4037,  ..., -0.0013, -0.0026,  0.0170])\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the document embedding is derived from word embeddings, its dimensionality depends on the dimensionality of word embeddings you are using.\n",
    "\n",
    "Next to the mean pooling operation you can also use min or max pooling. Simply pass the pooling operation you want to use to the initialization of the DocumentPoolEmbeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
    "                                             flair_embedding_backward,\n",
    "                                             flair_embedding_backward],\n",
    "                                             mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an RNN to obtain Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN takes the word embeddings of every token in the document as input and provides its last output state as document embedding. You can choose which type of RNN you wish to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a document embeddings RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings([glove_embedding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a GRU-type RNN is instantiated. Now, create an example sentence and call the embedding's embed() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Cho, et al. (2014) for GRU (Gated Recurrent Unit). It aims to solve the vanishing gradient problem which comes with a standard recurrent neural network (RNN). GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1233, -0.3052, -0.7384, -0.7638,  0.0027,  0.3457, -0.8189,  0.0000,\n",
      "         1.0095,  0.2369, -0.0000,  0.1885,  0.5334, -0.0000, -0.0000,  0.0083,\n",
      "         0.4750,  0.1510, -0.5622, -0.2329,  0.2736,  0.0000, -0.6329, -0.5956,\n",
      "         0.0000,  0.0000, -0.0000, -0.1479, -1.3262, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.3408,  0.0000, -0.0000, -0.2298, -0.2081,  0.5123, -0.0000,\n",
      "        -0.5355,  0.9092, -0.0000,  0.3954,  0.0000, -0.0000,  0.0000,  0.3119,\n",
      "         0.0000,  0.2398, -0.2036, -0.0813,  0.0000,  0.1089,  0.0000, -0.5947,\n",
      "        -0.0000, -0.4465,  0.2653, -0.0000,  0.0000, -0.0000,  0.0000,  0.3511,\n",
      "        -0.5406, -0.3120, -0.0000, -0.0000,  0.0000, -0.0000,  0.1125,  0.3827,\n",
      "        -0.7062,  0.0000,  0.5449, -0.6990,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "        -0.4847, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.1327,  0.0000,\n",
      "        -0.3140, -0.2876, -0.4289,  0.0000,  0.7989,  0.0000, -0.0000,  0.9270,\n",
      "         0.1857, -0.0000, -0.7961,  0.0000,  0.3700, -0.0000,  0.6803,  0.3176,\n",
      "        -0.0000, -0.2087, -0.0000,  0.0000, -1.0579, -0.3153,  0.1289, -0.4768,\n",
      "        -0.0000, -0.5298,  1.1000, -0.0000,  0.1561,  0.7618,  0.1268,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.1550,  0.1483,  0.0570],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a single embedding for the complete sentence. The embedding dimensionality depends on the number of hidden states you are using and whether the RNN is bidirectional or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a different type of RNN, you need to set the rnn_type parameter in the constructor. So, to initialize a document RNN embedding with an LSTM, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "document_lstm_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while DocumentPoolEmbeddings are immediately meaningful, DocumentRNNEmbeddings need to be tuned on the downstream task. This happens automatically in Flair, if you train a new model with these embeddings. Once the model is trained, you can access the tuned DocumentRNNEmbeddings object directly from the classifier object and use it to embed sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes word embeddings, puts them into an RNN to obtain a text representation, and puts the text representation in the end into a linear layer to get the actual class label. The model can handle single and multi class data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0000, -0.4872, -0.0000, -0.7042,  0.1878, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000,  0.1602, -0.7092,  0.0000,  0.0948, -0.7893, -0.0965, -0.0000,\n",
      "         0.4577,  0.0000, -0.2404, -0.5142,  0.0000,  0.2066, -0.6878, -0.0000,\n",
      "        -0.6402,  0.3356, -0.0000,  0.0000, -0.9928, -0.5057, -0.0000,  0.0719,\n",
      "        -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.1804,  0.8450, -0.0000,\n",
      "        -1.2277,  0.4854, -0.0000,  0.7708, -0.2778, -0.0000,  0.4839,  0.0876,\n",
      "         0.3991,  0.5270, -0.1792,  0.6602,  0.0000, -0.3155,  0.0000, -0.5706,\n",
      "        -0.0000, -0.0000, -0.3597,  0.0391,  0.0000, -0.1249,  0.5825,  0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.6441,  0.0957,  0.2359,\n",
      "        -0.7695,  0.0000,  0.3934, -0.0000, -0.0000,  0.0000,  0.7513,  0.0100,\n",
      "        -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.6134,  0.0184,  0.8898,\n",
      "        -0.0000, -0.2680,  0.4325, -0.2350,  0.5146,  0.0000, -0.2735,  0.0000,\n",
      "         0.1126,  0.0000, -0.0000, -0.0000,  1.0690, -0.0000,  0.7087, -0.0286,\n",
      "        -0.0000,  0.3027, -0.0000, -0.6103, -1.1207, -0.0000, -0.0840, -0.4544,\n",
      "        -0.0000, -0.0000,  0.6748, -0.0000,  0.0000,  0.8080,  0.0261,  0.5208,\n",
      "         0.2559, -0.3386, -1.3786, -0.0000,  0.0662,  0.5570,  0.5616, -0.0000],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "from flair.models import TextClassifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)\n",
    "document_embeddings = classifier.document_embeddings\n",
    "\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DocumentRNNEmbeddings have a number of hyper-parameters that can be tuned to improve learning:\n",
    "\n",
    "- hidden_size: the number of hidden states in the rnn.\n",
    "- rnn_layers: the number of layers for the rnn.\n",
    "- reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n",
    "layer before putting them into the rnn or not.\n",
    "- reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n",
    "dimension as before will be taken.\n",
    "- bidirectional: boolean value, indicating whether to use a bidirectional rnn or not.\n",
    "- dropout: the dropout value to be used.\n",
    "- word_dropout: the word dropout value to be used, if 0.0 word dropout is not used.\n",
    "- locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used.\n",
    "- rnn_type: one of 'RNN', 'LSTM', 'RNN_TANH' or 'RNN_RELU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our current example of Amazon reviews, we will use the following settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a classifier using the *document_embedding*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a trainer from the classifier and the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 02:41:21,074 No handles with labels found to put in legend.\n",
      "2019-04-10 02:41:21,083 No handles with labels found to put in legend.\n",
      "2019-04-10 02:41:21,094 No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves('resources/taggers/ag_news/loss.tsv')\n",
    "plotter.plot_weights('resources/taggers/ag_news/weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Flair_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
